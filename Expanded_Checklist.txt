Auto-Brainlift Expanded Build Checklist (Line-by-Line for Cursor)

==========================================================
⚠️ GENERAL PITFALLS – READ BEFORE BEGINNING ANY FEATURE
==========================================================

1. Always generate two files: `brainlift.md` and `context.md`
2. Do not overengineer LangGraph: linear node flow only
3. Distinguish tone: context.md = structured, brainlift.md = narrative
4. Use Electron UI only for simple buttons and text display
5. n8n should only trigger LangGraph, not contain logic
6. Output files go in `/brainlifts/` and `/context_logs/`
7. Never summarize during broken app states — trigger ONLY on commit
8. Log all prompt/output for debugging

==========================================================
1️⃣ PROJECT INITIALIZATION
==========================================================

- [x] Create the following folders at root level:
      - /agents/
      - /brainlifts/
      - /context_logs/
      - /logs/
      - /prompts/
      - /ui/
      - /workflows/

- [ ] Initialize npm and install Electron:
      npm init -y
      npm install electron --save-dev

- [ ] Set up `.env.template` with:
      OPENAI_API_KEY=
      OUTPUT_DIR=./brainlifts

- [ ] Create basic Electron shell:
      - main.ts / preload.js / index.html
      - Render "Generate Summary" button
      - Show last written file contents in UI

✅ Pitfall Guard:
> Keep the UI simple – no routing, no component trees. Just one page and a few elements.

==========================================================
1.5️⃣ PYTHON ENVIRONMENT SETUP
==========================================================

- [ ] Create Python virtual environment:
      python3 -m venv venv
      source venv/bin/activate  # On macOS/Linux

- [ ] Install Python dependencies:
      pip install langchain langgraph gitpython openai python-dotenv

- [ ] Create requirements.txt:
      langchain
      langgraph
      gitpython
      openai
      python-dotenv

- [ ] Configure OpenAI in .env:
      - Use GPT-4 or GPT-4-turbo for accuracy over speed
      - Set reasonable timeout and retry settings

✅ Pitfall Guard:
> Always use a virtual environment to avoid dependency conflicts. Document Python version (3.8+).

==========================================================
2️⃣ LANGGRAPH SUMMARIZATION AGENT
==========================================================

- [ ] Create `langgraph_agent.py` in /agents/
- [ ] Create these LangGraph nodes:
      - parse_git_diff
      - summarize_context
      - summarize_brainlift
      - write_output

- [ ] Chain nodes in a linear graph:
      parse_git_diff → summarize_context → summarize_brainlift → write_output

- [ ] Create a dummy git diff file and pass to the agent

- [ ] Store two prompt templates in `/prompts/`:
      - brainlift.txt
      - context.txt

✅ Pitfall Guard:
> Don't use nested graphs or unnecessary branching. Each input must result in two markdown files.

==========================================================
3️⃣ GIT COMMIT INTEGRATION
==========================================================

- [ ] Use GitPython (or `simple-git`) to read last commit diff
- [ ] Save commit hash after summary to avoid repeats
- [ ] Extract diff and format for LangGraph input
- [ ] Test: after each commit, LangGraph is triggered with correct diff

- [ ] Create a `post-commit` Git hook that:
      - Reads last commit diff
      - Triggers langgraph_agent.py

✅ Pitfall Guard:
> Only trigger summarization on commits – not on saves or idle. This avoids summarizing half-baked code.

==========================================================
4️⃣ AI PROMPTS AND OUTPUT WRITING
==========================================================

- [ ] Read prompts from `/prompts/*.txt`
- [ ] Use OpenAI/Claude to generate:
      - context.md (structured format)
      - brainlift.md (reflective format)

- [ ] Write files with filenames:
      - /brainlifts/YYYY-MM-DD_HH-MM-SS_brainlift.md
      - /context_logs/YYYY-MM-DD_HH-MM-SS_context.md

✅ Pitfall Guard:
> Always generate both files. Never write to root or overwrite without checking.

==========================================================
5️⃣ ELECTRON ↔ LANGGRAPH TRIGGER
==========================================================

- [ ] Use Node `child_process.spawn` to run langgraph_agent.py
- [ ] Hook up Electron UI "Generate" button to manual summary trigger
- [ ] Display output contents in a preview pane

✅ Pitfall Guard:
> Don't try to do AI in the renderer. Just call your Python agent.

==========================================================
6️⃣ OPTIONAL: N8N TRIGGER (FUTURE ENHANCEMENT)
==========================================================

- [ ] Set up local n8n instance
- [ ] Create webhook or folder trigger
- [ ] Point it to run langgraph_agent.py via command

✅ Pitfall Guard:
> n8n does not contain logic – it only detects commit or file event and passes control.

==========================================================
7️⃣ LOGGING & ERROR HANDLING
==========================================================

- [ ] Write AI prompts and responses to `/logs/debug.log`
- [ ] If generation fails, show message in UI
- [ ] Optionally include commit hash in file metadata
- [ ] Implement retry queue for failed API calls:
      - Alert user on failure
      - Queue failed attempts
      - Retry with exponential backoff
      - Maximum 3 retry attempts

✅ Pitfall Guard:
> Always log. Debugging LangGraph without logs is painful.

==========================================================
8️⃣ EXPANSION: UI ENHANCEMENTS & MULTI-PROJECT SUPPORT
==========================================================

==========================================================
8️⃣ UI ENHANCEMENTS & MULTI-PROJECT SUPPORT
==========================================================

- [x] Add project management UI controls:
      - "Change Project Directory" button
      - "Add New Project" button  
      - "Switch Project" dropdown
      - "View Project History" button
      - "Settings" button (API keys, budgets)

- [x] Create project state display:
      - Current project path indicator
      - Token usage meter (visual progress bar)
      - Cost tracker showing "$X.XX spent today"
      - Cache hit rate percentage display

- [x] Implement quick actions bar:
      - "Generate Summary Now" override button
      - "Clear Cache" with confirmation
      - "Export Summaries" to CSV/JSON

- [x] Set up project registry (~/.auto-brainlift/projects.json):
      - Unique project IDs
      - Project paths and names
      - Last processed commit hash
      - Per-project settings (token budgets, enabled agents)

- [x] Create per-project storage structure:
      ~/.auto-brainlift/
      ├── projects.json
      ├── cache/
      │   └── project-id/
      │       ├── semantic.db
      │       └── exact.db
      └── outputs/
          └── project-id/

✅ Pitfall Guard:
> Store all project data outside Git repos to avoid conflicts. Use UUIDs for project IDs.

==========================================================
9️⃣ SMART CACHING & COST OPTIMIZATION
==========================================================

- [x] Implement 3-tier caching system:
      - Level 1: Exact match cache (50ms response)
      - Level 2: Semantic cache (2s response)
      - Level 3: Full LLM processing (6s response)

- [x] Set up exact match cache:
      - In-memory hashmap for speed
      - MD5 hash of queries as keys
      - TTL of 3600 seconds (1 hour)

- [x] Create semantic cache with embeddings:
      - Use text-embedding-ada-002 ($0.0001/1K tokens)
      - Store embeddings in vector database
      - Similarity threshold of 0.85
      - TTL of 86400 seconds (24 hours)

- [x] Build smart query router:
      - Check exact cache first
      - Fall back to semantic cache
      - Use GPT-3.5 for simple tasks
      - Reserve GPT-4 for complex analysis

- [x] Implement diff chunking:
      - 1000 character chunks
      - 100 character overlap
      - Preserve semantic boundaries (functions, classes)

- [x] Add token budget manager:
      - User-configurable per-commit limits (1k-50k tokens)
      - Enable/disable toggle for testing
      - Estimated cost preview before processing
      - Visual budget status indicators
      - "Bypass Budget" developer mode

✅ Pitfall Guard:
> Always check cheaper options first. Cache embeddings permanently as they're cheap to store.
> FIXED: Path mismatch between Python (~/.config) and Electron (~/Library/Application Support)
> FIXED: Import statements to use proper module paths
> FIXED: Cache stats now show real data, not mock values

==========================================================
🔟 CONFIGURABLE MULTI-AGENT SYSTEM
==========================================================

- [x] Create base agent framework:
      - Abstract agent class (BaseAgent, SpecializedAgent)
      - Shared state management (AgentState)
      - Result aggregation

- [x] Implement specialized agents:
      - Security scanner (patterns + LLM)
      - Code quality analyzer
      - Documentation generator
      - Each with cost estimates

- [x] Build agent configuration UI:
      - Toggle switches per agent
      - Cost preview per agent
      - Execution mode selector (parallel/sequential/priority)
      - Model selection per agent (GPT-4/GPT-3.5)

- [x] Add budget allocation system:
      - Integration with existing budget manager
      - Token tracking per agent
      - Cost estimation in UI

- [x] Create orchestration logic:
      - Parallel execution when budget allows
      - Sequential fallback for tight budgets
      - Priority-based execution mode
      - Result merging and deduplication

- [x] Generate detailed error logs:
      - Separate error_log.md file with all issues
      - Structured format with recommendations
      - Severity levels and impact assessment

- [x] Implement flexible document viewer:
      - Dropdown selectors for document type
      - Historical file browser
      - Mix-and-match panel viewing

✅ Pitfall Guard:
> Keep agents independent. One agent failure shouldn't break others.
> FIXED: Python imports when running from Electron (PYTHONPATH + sys.path)
> FIXED: Cache workflow to run agents BEFORE generating summaries
> FIXED: Agent results now properly integrated into output files

==========================================================
1️⃣1️⃣ PRODUCTION HARDENING & MONITORING
==========================================================

- [ ] Implement comprehensive error handling:
      - Try-catch around all LLM calls
      - Graceful degradation to cached results
      - User-friendly error messages

- [ ] Add retry logic:
      - Exponential backoff (1, 2, 4, 8 seconds)
      - Maximum 3 retry attempts
      - Different endpoints on retry

- [ ] Set resource limits:
      - Max file size: 10MB
      - Max diff size: 100KB
      - LLM timeout: 30 seconds

- [ ] Create usage analytics dashboard:
      - Daily/weekly/monthly API calls
      - Cost breakdown by agent
      - Cache hit rates graph
      - Error rate tracking

- [ ] Build export functionality:
      - CSV export for accounting
      - JSON export for analysis
      - Automated monthly reports

- [ ] Optimize performance:
      - Parallel diff processing
      - File type filtering (.jpg, .png skip)
      - Background agent processing

✅ Pitfall Guard:
> Monitor everything but store metrics locally. Never send usage data to external services.

==========================================================
1️⃣2️⃣ TESTING & VALIDATION
==========================================================

- [ ] Test multi-project scenarios:
      - Switching between 5+ projects rapidly
      - Concurrent commits in different projects
      - Project deletion and recreation

- [ ] Validate caching effectiveness:
      - Measure hit rates (target >60%)
      - Verify cost reduction (target 10x)
      - Test cache invalidation

- [ ] Load test the system:
      - 1000+ commits simulation
      - Large files (5MB+)
      - Concurrent operations

- [ ] Test failure scenarios:
      - API downtime
      - Rate limiting
      - Network failures
      - Corrupted cache

- [ ] Platform testing:
      - Windows path handling
      - macOS permissions
      - Linux Python paths

✅ Pitfall Guard:
> Test edge cases early. It's easier to fix architecture issues before launch.

==========================================================
1️⃣3️⃣ FINAL POLISH AND SUBMISSION
==========================================================

- [ ] Update documentation:
      - New UI features guide
      - Multi-project setup
      - Caching explanation
      - Cost optimization tips

- [ ] Create demo materials:
      - Video showing multi-project switching
      - Screenshot of cost savings
      - Performance comparison graphs

- [ ] Package for distribution:
      - Include default settings
      - First-run setup wizard
      - Migration from v1 guide

- [ ] Final checklist:
      - All tests passing
      - Documentation complete
      - No hardcoded values
      - Clean Git history

✅ Pitfall Guard:
> Don't rush the polish. A smooth user experience is worth the extra effort.